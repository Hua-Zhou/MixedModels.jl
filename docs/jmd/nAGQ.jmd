
# Parameter estimation in GLMMs

The [`lme4`](https://github.com/lme4/lme4) package for [`R`](https://www.r-project.org) provides the `glmer` function to define and fit generalized linear mixed models (GLMMs).  The [`MixedModels`](https://github.com/dmbates/MixedModels.jl) package for [`Julia`](https://julialang.org) provides a similar function called `glmm`.  Because I am more familiar with `glmm` than with `glmer` these days, I will explain the algorithms using `glmm` for illustration.

This is a [Jupyter](https://jupyter.org) notebook of generalized linear mixed models (GLMM's) fit to data from a perception study data using the [`MixedModels`](https://github.com/dmbates/MixedModels.jl) package for [Julia](https://julialang.org).

The notebook can be run by [downloading](https://julialang.org/downloads) version 0.6.0 or later of Julia and installing the `IJulia` and `MixedModels` packages.  The Julia function to install a package is `Pkg.add()`, e.g.
```jl
Pkg.add("MixedModels")
```

When `IJulia` is installed you can start a notebook server in your browser by starting Julia in the usual way and, from within Julia, running
```jl
using IJulia
notebook()
```

Navigate to the location of this notebook file and select it to start the notebook.

An alternative is to log in to https://juliabox.com using one of the login options.  From the `File` tab, upload the data and this notebook.  Returning to the `Jupyter` tab and selecting the notebook should start it.

Once the notebook is running,  `Ctrl-Enter` runs a cell or `Shift-Enter` runs a cell and moves to the next cell.  Check under `Help -> Keyboard Shortcuts` for other key sequences.

## Making packages available

The Julia equivalent of R's `library()` function is the `using` directive.

```julia
using DataFrames, MixedModels, RData
```

Consider the `VerbAgg` data from the `lme4` package.  This is one of the data sets available in the `test/dat.rda` file for the `MixedModels` package

```julia
const dat = convert(Dict{Symbol,DataFrame},
    load(Pkg.dir("MixedModels", "test", "dat.rda")))
```

```julia
const verbagg = dat[:VerbAgg]
```

At present the response for a Bernoulli GLMM must be a 0/1 floating point vector so we convert the factor `r2`.

```julia
verbagg[:r201] = Float64[r == "Y" for r in verbagg[:r2]]
```

## Details of evaluating the objective function

The `glmm` function generates, but does not fit, a `GeneralizedLinearMixedModel` object.

```julia
mdl = glmm(@formula(r201 ~ 1 + a + g + b + s + (1|id) + (1|item)),
    verbagg, Bernoulli());
typeof(mdl)
```

A separate call to `fit!` is required to fit the model.  This involves optimizing an objective function, the Laplace approximation to the deviance, with respect to the parameters, which are $\beta$, the fixed-effects coefficients, and $\theta$, the covariance parameters.  The starting estimate for $\beta$ is determined by fitting a GLM to the fixed-effects part of the formula

```julia
mdl.β
```

and the starting estimate for $\theta$, which is a vector of the two standard deviations of the random effects, is chosen to be

```julia
mdl.θ
```

The Laplace approximation to the deviance requires determining the conditional modes of the random effects.  These are the values that maximize the conditional density of the random effects, given the model parameters and the data.  This is done using Penalized Iteratively Reweighted Least Squares (PIRLS).  In most cases PIRLS is fast and stable.  It is simply a penalized version of the IRLS algorithm used in fitting GLMs.

The distinction between the "fast" and "slow" algorithms in the `MixedModels` package (`nAGQ=0` or `nAGQ=1` in `lme4`) is whether the fixed-effects parameters, $\beta$, are optimized in PIRLS or in the nonlinear optimizer. In a call to the `pirls!` function the first argument is a `GeneralizedLinearMixedModel`, which is modified during the function call.  (By convention, the names of such *mutating functions* end in `!` as a warning to the user that they can modify an argument, usually the first argument.)  The second and third arguments are optional logical values indicating if $\beta$ is to be varied and if verbose output is to be printed.

```julia
pirls!(mdl, true, true)
```

```julia
LaplaceDeviance(mdl)
```

```julia
mdl.β
```

```julia
mdl.θ # current values of the standard deviations of the random effects
```

If the optimization with respect to $\beta$ is performed within PIRLS then the nonlinear optimization of the Laplace approximation to the deviance requires optimization with respect to $\theta$ only. This is the "fast" algorithm. Given a value of $\theta$ PIRLS is used to determine the conditional estimate of $\beta$ and the conditional mode of the random effects, **b**.

```julia
mdl.b # conditional modes of b
```

```julia
fit!(mdl, fast=true, verbose=true);
```

The optimization process is summarized by

```julia
mdl.LMM.optsum
```

As one would hope, given the name of the option, this fit is fast.

```julia
@time(fit!(glmm(@formula(r201 ~ 1 + a + g + b + s + (1 | id) + (1 | item)), 
        verbagg, Bernoulli()), fast=true))
```

The alternative algorithm is to use PIRLS to find the conditional mode of the random effects, given $\beta$ and $\theta$ and then use the general nonlinear optimizer to fit with respect to both $\beta$ and $\theta$.  Because it is slower to incorporate the $\beta$ parameters in the general nonlinear optimization, the fast fit is performed first and used to determine starting estimates for the more general optimization.

```julia
@time mdl1 = fit!(glmm(@formula(r201 ~ 1+a+g+b+s+(1|id)+(1|item)), 
        verbagg, Bernoulli()), verbose = true)
```

This fit provided slightly better results (Laplace approximation to the deviance of 8151.400 versus 8151.583) but took 6 times as long.  That is not terribly important when the times involved are a few seconds but can be important when the fit requires many hours or days of computing time.

The comparison of the slow and fast fit is available in the optimization summary after the slow fit.

```julia
mdl1.LMM.optsum
```
