
# Parameter estimation in GLMMs

The [`lme4`](https://github.com/lme4/lme4) package for [`R`](https://www.r-project.org) provides the `glmer` function to define and fit generalized linear mixed models (GLMMs).
The [`MixedModels`](https://github.com/dmbates/MixedModels.jl) package for [`Julia`](https://julialang.org) provides a similar function called `glmm`.
Because I am more familiar with `glmm` than with `glmer` these days, I will explain the algorithms using `glmm` for illustration.

## Making packages available

The Julia equivalent of R's `library()` function is the `using` directive.

```{julia;term=true}
using DataFrames, MixedModels, RData
```

Consider the `VerbAgg` data from the `lme4` package.  This is one of the data sets available in the `test/dat.rda` file for the `MixedModels` package

```{julia;term=true}
const dat = convert(Dict{Symbol,DataFrame}, load(Pkg.dir("MixedModels", "test", "dat.rda")));
```

## Details of evaluating the objective function

The `glmm` function generates, but does not fit, a `GeneralizedLinearMixedModel` object.

```{julia;term=true}
mdl = glmm(@formula(r2 ~ 1 + a + g + b + s + (1|id) + (1|item)),
           dat[:VerbAgg], Bernoulli());
typeof(mdl)
```

A separate call to `fit!` is required to fit the model.
This involves optimizing an objective function, the Laplace approximation to the deviance, with respect to the parameters, which are $\beta$, the fixed-effects coefficients, and $\theta$, the covariance parameters.  The starting estimate for $\beta$ is determined by fitting a GLM to the fixed-effects part of the formula

```{julia;term=true}
mdl.β
```

and the starting estimate for $\theta$, which is a vector of the two standard deviations of the random effects, is chosen to be

```{julia;term=true}
mdl.θ
```

The Laplace approximation to the deviance requires determining the conditional modes of the random effects.
These are the values that maximize the conditional density of the random effects, given the model parameters and the data.
This is done using Penalized Iteratively Reweighted Least Squares (PIRLS).
In most cases PIRLS is fast and stable.
It is simply a penalized version of the IRLS algorithm used in fitting GLMs.

The distinction between the "fast" and "slow" algorithms in the `MixedModels` package (`nAGQ=0` or `nAGQ=1` in `lme4`) is whether the fixed-effects parameters, $\beta$, are optimized in PIRLS or in the nonlinear optimizer.
In a call to the `pirls!` function the first argument is a `GeneralizedLinearMixedModel`, which is modified during the function call.
(By convention, the names of such *mutating functions* end in `!` as a warning to the user that they can modify an argument, usually the first argument.)
The second and third arguments are optional logical values indicating if $\beta$ is to be varied and if verbose output is to be printed.

```{julia;term=true}
pirls!(mdl, true, true)
```

```{julia;term=true}
LaplaceDeviance(mdl)
```

```{julia;term=true}
mdl.β
```

```{julia;term=true}
mdl.θ # current values of the standard deviations of the random effects
```

If the optimization with respect to $\beta$ is performed within PIRLS then the nonlinear optimization of the Laplace approximation to the deviance requires optimization with respect to $\theta$ only.
This is the "fast" algorithm.
Given a value of $\theta$ PIRLS is used to determine the conditional estimate of $\beta$ and the conditional mode of the random effects, **b**.

```{julia;term=true}
mdl.b # conditional modes of b
```

```{julia;term=true}
fit!(mdl, fast=true, verbose=true);
```

The optimization process is summarized by

```{julia;term=true}
mdl.LMM.optsum
```

As one would hope, given the name of the option, this fit is fast.

```{julia;term=true}
@time(fit!(glmm(@formula(r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)), 
        dat[:VerbAgg], Bernoulli()), fast=true))
```

The alternative algorithm is to use PIRLS to find the conditional mode of the random effects, given $\beta$ and $\theta$ and then use the general nonlinear optimizer to fit with respect to both $\beta$ and $\theta$.
Because it is slower to incorporate the $\beta$ parameters in the general nonlinear optimization, the fast fit is performed first and used to determine starting estimates for the more general optimization.

```{julia;term=true}
@time mdl1 = fit!(glmm(@formula(r2 ~ 1+a+g+b+s+(1|id)+(1|item)), 
        dat[:VerbAgg], Bernoulli()), verbose = true)
```

This fit provided slightly better results (Laplace approximation to the deviance of 8151.400 versus 8151.583) but took 6 times as long.
That is not terribly important when the times involved are a few seconds but can be important when the fit requires many hours or days of computing time.

The comparison of the slow and fast fit is available in the optimization summary after the slow fit.

```{julia;term=true}
mdl1.LMM.optsum
```
