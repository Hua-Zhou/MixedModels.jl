<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Parameter estimation in GLMMs · MixedModels</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><a href="index.html"><img class="logo" src="assets/logo.png" alt="MixedModels logo"/></a><h1>MixedModels</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">MixedModels.jl Documentation</a></li><li><a class="toctext" href="constructors.html">Model constructors</a></li><li><a class="toctext" href="optimization.html">Details of the parameter estimation</a></li><li><a class="toctext" href="bootstrap.html">Parametric bootstrap for linear mixed-effects models</a></li><li><a class="toctext" href="SimpleLMM.html">A Simple, Linear, Mixed-effects Model</a></li><li><a class="toctext" href="MultipleTerms.html">Models With Multiple Random-effects Terms</a></li><li class="current"><a class="toctext" href="nAGQ.html">Parameter estimation in GLMMs</a><ul class="internal"><li><a class="toctext" href="#Making-packages-available-1">Making packages available</a></li><li><a class="toctext" href="#Details-of-evaluating-the-objective-function-1">Details of evaluating the objective function</a></li></ul></li><li><a class="toctext" href="SingularCovariance.html">Singular covariance estimates in random regression models</a></li><li><a class="toctext" href="SubjectItem.html">-</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="nAGQ.html">Parameter estimation in GLMMs</a></li></ul><a class="edit-page" href="https://github.com/dmbates/MixedModels.jl/blob/master/docs/src/nAGQ.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Parameter estimation in GLMMs</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Parameter-estimation-in-GLMMs-1" href="#Parameter-estimation-in-GLMMs-1">Parameter estimation in GLMMs</a></h1><p>The <a href="https://github.com/lme4/lme4"><code>lme4</code></a> package for <a href="https://www.r-project.org"><code>R</code></a> provides the <code>glmer</code> function to define and fit generalized linear mixed models (GLMMs). The <a href="https://github.com/dmbates/MixedModels.jl"><code>MixedModels</code></a> package for <a href="https://julialang.org"><code>Julia</code></a> provides a similar function called <code>glmm</code>. Because I am more familiar with <code>glmm</code> than with <code>glmer</code> these days, I will explain the algorithms using <code>glmm</code> for illustration.</p><h2><a class="nav-anchor" id="Making-packages-available-1" href="#Making-packages-available-1">Making packages available</a></h2><p>The Julia equivalent of R&#39;s <code>library()</code> function is the <code>using</code> directive.</p><pre><code class="language-julia">julia&gt; using DataFrames, MixedModels, RData
</code></pre><p>Consider the <code>VerbAgg</code> data from the <code>lme4</code> package.  This is one of the data sets available in the <code>test/dat.rda</code> file for the <code>MixedModels</code> package</p><pre><code class="language-julia">julia&gt; const dat = convert(Dict{Symbol,DataFrame}, load(Pkg.dir(&quot;MixedModels&quot;, &quot;test&quot;, &quot;dat.rda&quot;)));
</code></pre><h2><a class="nav-anchor" id="Details-of-evaluating-the-objective-function-1" href="#Details-of-evaluating-the-objective-function-1">Details of evaluating the objective function</a></h2><p>The <code>glmm</code> function generates, but does not fit, a <code>GeneralizedLinearMixedModel</code> object.</p><pre><code class="language-julia">julia&gt; mdl = glmm(@formula(r2 ~ 1 + a + g + b + s + (1|id) + (1|item)),
           dat[:VerbAgg], Bernoulli());

julia&gt; typeof(mdl)
MixedModels.GeneralizedLinearMixedModel{Float64}
</code></pre><p>A separate call to <code>fit!</code> is required to fit the model. This involves optimizing an objective function, the Laplace approximation to the deviance, with respect to the parameters, which are <span>$\beta$</span>, the fixed-effects coefficients, and <span>$\theta$</span>, the covariance parameters.  The starting estimate for <span>$\beta$</span> is determined by fitting a GLM to the fixed-effects part of the formula</p><pre><code class="language-julia">julia&gt; mdl.β
6-element Array{Float64,1}:
  0.206053 
  0.0399404
  0.231317 
 -0.794186 
 -1.53919  
 -0.776656 
</code></pre><p>and the starting estimate for <span>$\theta$</span>, which is a vector of the two standard deviations of the random effects, is chosen to be</p><pre><code class="language-julia">julia&gt; mdl.θ
2-element Array{Float64,1}:
 1.0
 1.0
</code></pre><p>The Laplace approximation to the deviance requires determining the conditional modes of the random effects. These are the values that maximize the conditional density of the random effects, given the model parameters and the data. This is done using Penalized Iteratively Reweighted Least Squares (PIRLS). In most cases PIRLS is fast and stable. It is simply a penalized version of the IRLS algorithm used in fitting GLMs.</p><p>The distinction between the &quot;fast&quot; and &quot;slow&quot; algorithms in the <code>MixedModels</code> package (<code>nAGQ=0</code> or <code>nAGQ=1</code> in <code>lme4</code>) is whether the fixed-effects parameters, <span>$\beta$</span>, are optimized in PIRLS or in the nonlinear optimizer. In a call to the <code>pirls!</code> function the first argument is a <code>GeneralizedLinearMixedModel</code>, which is modified during the function call. (By convention, the names of such <em>mutating functions</em> end in <code>!</code> as a warning to the user that they can modify an argument, usually the first argument.) The second and third arguments are optional logical values indicating if <span>$\beta$</span> is to be varied and if verbose output is to be printed.</p><pre><code class="language-julia">julia&gt; pirls!(mdl, true, true)
varyβ = true
obj₀ = 10210.8534389054
β = [0.206053, 0.0399404, 0.231317, -0.794186, -1.53919, -0.776656]
iter = 1
obj = 8301.483049027265
iter = 2
obj = 8205.604285133919
iter = 3
obj = 8201.896597466888
iter = 4
obj = 8201.848598910705
iter = 5
obj = 8201.848559060703
8201.848559060703
</code></pre><pre><code class="language-julia">julia&gt; LaplaceDeviance(mdl)
8201.848559060703
</code></pre><pre><code class="language-julia">julia&gt; mdl.β
6-element Array{Float64,1}:
  0.218535 
  0.0514385
  0.290225 
 -0.979124 
 -1.95402  
 -0.979493 
</code></pre><pre><code class="language-julia">julia&gt; mdl.θ # current values of the standard deviations of the random effects
2-element Array{Float64,1}:
 1.0
 1.0
</code></pre><p>If the optimization with respect to <span>$\beta$</span> is performed within PIRLS then the nonlinear optimization of the Laplace approximation to the deviance requires optimization with respect to <span>$\theta$</span> only. This is the &quot;fast&quot; algorithm. Given a value of <span>$\theta$</span> PIRLS is used to determine the conditional estimate of <span>$\beta$</span> and the conditional mode of the random effects, <strong>b</strong>.</p><pre><code class="language-julia">julia&gt; mdl.b # conditional modes of b
2-element Array{Array{Float64,2},1}:
 [-0.600772 -1.93227 -0.0601337 0.455415 -0.0601337 -0.0592371 -0.30784 -1.35294 -0.358272 1.80735 0.885198 1.12211 -1.11049 1.12236 0.495591 0.239355 0.546633 -0.191338 -2.39926 -0.0592457 -0.622114 2.07196 1.89528 -0.0641204 -1.26575 -1.19175 -0.593128 1.14862 0.0422574 1.16389 1.27252 1.50087 -0.811902 -0.318641 -0.0592457 2.23746 -1.4786 -0.467503 -1.16637 -2.07574 -0.0165811 -0.400881 -1.76538 -0.551235 -0.844867 1.27252 1.35156 -1.59171 -0.57348 -0.485627 -1.26823 0.504362 1.71923 -0.272538 -0.748993 2.39518 0.20067 1.47384 -0.933614 -1.02185 0.983772 1.6407 -0.144554 0.853269 -0.102709 -1.13932 -0.187214 2.63091 0.0260731 0.110112 -1.19175 -0.0581051 -1.76538 -0.145291 -1.65537 -0.951261 -1.02185 -1.85832 -0.860825 -0.385634 -1.29466 1.1899 -0.400881 -0.676971 0.419856 -1.70552 1.2312 0.800493 -0.4733 -1.37962 -1.61581 -1.90904 0.127412 -1.54081 0.955177 0.582894 -2.01224 0.370468 -1.11141 -1.23199 0.0270239 -1.1449 0.455415 -0.326504 1.55552 -0.148923 -1.0317 1.31086 -1.49747 -1.82001 -1.23199 -1.23199 0.495591 0.154033 1.71865 0.419856 0.673514 0.0687165 -1.17128 0.519487 0.196693 0.285552 -0.990087 2.40442 -0.403554 0.452674 -0.551235 -0.0592371 -0.899086 0.727151 0.97952 1.88793 -1.28187 -1.69498 0.885198 -1.3094 -1.19175 0.0413243 0.0974061 -1.34045 -0.676971 0.788556 0.758156 0.538004 2.52793 -0.357865 -0.400881 0.752965 -0.0601337 -1.02185 0.916215 0.97952 0.470617 0.970099 0.916215 -1.35294 -0.495346 0.631209 -0.368694 1.55552 1.01181 -1.07266 0.0422574 0.196693 -0.233763 0.455415 -0.257214 -0.187877 0.0839808 -0.106517 -0.106517 0.61619 -0.488501 -0.728311 0.370468 0.727151 2.26221 -0.446023 0.4979 0.727151 -1.37888 -0.276198 0.666102 0.452972 -1.35294 -0.760884 0.495591 0.127412 0.127412 -0.661964 -1.98915 -0.144554 0.285552 -0.318641 3.01725 -0.875977 -1.39182 -0.606988 -0.606988 -0.0175635 1.52097 0.215855 1.03969 -0.53759 -0.850511 -0.0439729 0.58084 -1.59171 1.53806 0.910396 0.574223 1.91443 0.370468 -0.776166 0.658174 -0.657997 0.700173 0.532272 1.53806 1.04137 1.87518 1.79681 0.0260731 -0.318641 0.468222 -0.272538 0.71583 0.640614 0.452674 0.71583 0.800493 -1.17128 1.12211 0.512612 1.31387 -0.782349 -0.53759 -1.23199 -1.07131 -2.26277 -0.823849 1.63732 0.758156 0.559209 -1.85832 -0.102709 0.013146 -1.51623 -1.41831 0.540393 -2.12861 1.59592 -0.495346 -0.214573 0.111374 -0.63504 0.758156 -0.564723 1.61572 1.89528 0.71583 2.22592 2.12281 1.60657 0.243107 0.370468 0.0675594 -0.844867 -0.233763 -0.453115 0.71583 1.44666 0.61619 0.243107 0.811215 -0.326504 -0.453115 -0.230469 -1.41884 -0.699423 1.14862 -0.558273 -0.733854 1.12211 0.937422 0.428137 -0.802867 0.540393 0.285552 -0.4733 0.0734134 -0.229832 1.36695 1.51467 -0.833638 -1.75866 -1.27226 -0.871847 0.282017 0.196669 -0.906921 -1.26823 0.773305 -0.257822 0.673514 1.72793 -0.593128 0.623469 -0.284328 0.0260606 0.0885882 0.561762 -0.187877 -2.53791 -0.144554 -0.575224]
 [-0.186364 0.180552 0.699755 0.290529 0.310311 0.637592 0.14885 0.00760348 0.203228 0.649678 0.313762 0.641038 -0.186364 0.0214228 -0.164584 -0.502754 -0.384328 -0.715767 -0.524248 -0.731414 -1.07929 0.310673 0.282092 -0.221974]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
</code></pre><pre><code class="language-julia">julia&gt; fit!(mdl, fast=true, verbose=true);
f_1: 8201.84856 [1.0, 1.0]
f_2: 8190.11782 [1.75, 1.0]
f_3: 8224.45098 [1.0, 1.75]
f_4: 9026.00391 [0.25, 1.0]
f_5: 8205.79378 [1.0, 0.25]
f_6: 8157.04103 [1.38583, 0.736457]
f_7: 8367.72422 [1.33715, 0.0]
f_8: 8170.28883 [1.41365, 1.11042]
f_9: 8158.82932 [1.27225, 0.762811]
f_10: 8161.93341 [1.40936, 0.868084]
f_11: 8156.30098 [1.32694, 0.721015]
f_12: 8156.11668 [1.32365, 0.714275]
f_13: 8156.00207 [1.31847, 0.708856]
f_14: 8155.75359 [1.32072, 0.701702]
f_15: 8155.27522 [1.32636, 0.687802]
f_16: 8154.41 [1.33859, 0.660408]
f_17: 8153.39496 [1.37582, 0.613358]
f_18: 8152.74094 [1.39515, 0.563096]
f_19: 8151.76473 [1.36763, 0.509124]
f_20: 8152.80894 [1.26776, 0.475123]
f_21: 8152.86642 [1.4148, 0.471099]
f_22: 8151.76959 [1.32589, 0.527523]
f_23: 8151.73776 [1.36681, 0.498606]
f_24: 8151.58516 [1.33974, 0.493492]
f_25: 8151.60206 [1.33758, 0.486311]
f_26: 8151.6005 [1.34692, 0.491348]
f_27: 8151.58338 [1.33958, 0.497337]
f_28: 8151.58359 [1.33927, 0.49802]
f_29: 8151.58335 [1.33972, 0.496955]
f_30: 8151.58351 [1.34041, 0.497255]
f_31: 8151.5834 [1.33957, 0.49622]
f_32: 8151.58348 [1.34031, 0.496495]
f_33: 8151.58334 [1.33956, 0.496833]
f_34: 8151.58334 [1.33953, 0.496902]
f_35: 8151.58334 [1.33963, 0.496867]
f_36: 8151.58334 [1.33963, 0.496803]
f_37: 8151.58334 [1.33956, 0.496833]
</code></pre><p>The optimization process is summarized by</p><pre><code class="language-julia">julia&gt; mdl.LMM.optsum
Initial parameter vector: [1.0, 1.0]
Initial objective value:  8201.848559060625

Optimizer (from NLopt):   LN_BOBYQA
Lower bounds:             [0.0, 0.0]
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10]
initial_step:             [0.75, 0.75]
maxfeval:                 -1

Function evaluations:     37
Final parameter vector:   [1.33956, 0.496833]
Final objective value:    8151.583340132135
Return code:              FTOL_REACHED

</code></pre><p>As one would hope, given the name of the option, this fit is fast.</p><pre><code class="language-julia">julia&gt; @time(fit!(glmm(@formula(r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)), 
        dat[:VerbAgg], Bernoulli()), fast=true))
  0.303345 seconds (117.33 k allocations: 12.741 MiB, 9.12% gc time)
Generalized Linear Mixed Model fit by minimizing the Laplace approximation to the deviance
  Formula: r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)
  Distribution: Distributions.Bernoulli{Float64}
  Link: GLM.LogitLink()

  Deviance (Laplace approximation): 8151.5833

Variance components:
          Column    Variance   Std.Dev. 
 id   (Intercept)  1.79443144 1.3395639
 item (Intercept)  0.24684282 0.4968328

 Number of obs: 7584; levels of grouping factors: 316, 24

Fixed-effects parameters:
              Estimate Std.Error  z value P(&gt;|z|)
(Intercept)   0.208273  0.405425 0.513715  0.6075
a            0.0543791 0.0167533  3.24587  0.0012
g: M          0.304089  0.191223  1.59023  0.1118
b: scold       -1.0165  0.257531 -3.94708   &lt;1e-4
b: shout       -2.0218  0.259235 -7.79912  &lt;1e-14
s: self       -1.01344  0.210888 -4.80559   &lt;1e-5

</code></pre><p>The alternative algorithm is to use PIRLS to find the conditional mode of the random effects, given <span>$\beta$</span> and <span>$\theta$</span> and then use the general nonlinear optimizer to fit with respect to both <span>$\beta$</span> and <span>$\theta$</span>. Because it is slower to incorporate the <span>$\beta$</span> parameters in the general nonlinear optimization, the fast fit is performed first and used to determine starting estimates for the more general optimization.</p><pre><code class="language-julia">julia&gt; @time mdl1 = fit!(glmm(@formula(r2 ~ 1+a+g+b+s+(1|id)+(1|item)), 
        dat[:VerbAgg], Bernoulli()), verbose = true)
f_1: 8201.84856 [1.0, 1.0]
f_2: 8190.11782 [1.75, 1.0]
f_3: 8224.45098 [1.0, 1.75]
f_4: 9026.00391 [0.25, 1.0]
f_5: 8205.79378 [1.0, 0.25]
f_6: 8157.04103 [1.38583, 0.736457]
f_7: 8367.72422 [1.33715, 0.0]
f_8: 8170.28883 [1.41365, 1.11042]
f_9: 8158.82932 [1.27225, 0.762811]
f_10: 8161.93341 [1.40936, 0.868084]
f_11: 8156.30098 [1.32694, 0.721015]
f_12: 8156.11668 [1.32365, 0.714275]
f_13: 8156.00207 [1.31847, 0.708856]
f_14: 8155.75359 [1.32072, 0.701702]
f_15: 8155.27522 [1.32636, 0.687802]
f_16: 8154.41 [1.33859, 0.660408]
f_17: 8153.39496 [1.37582, 0.613358]
f_18: 8152.74094 [1.39515, 0.563096]
f_19: 8151.76473 [1.36763, 0.509124]
f_20: 8152.80894 [1.26776, 0.475123]
f_21: 8152.86642 [1.4148, 0.471099]
f_22: 8151.76959 [1.32589, 0.527523]
f_23: 8151.73776 [1.36681, 0.498606]
f_24: 8151.58516 [1.33974, 0.493492]
f_25: 8151.60206 [1.33758, 0.486311]
f_26: 8151.6005 [1.34692, 0.491348]
f_27: 8151.58338 [1.33958, 0.497337]
f_28: 8151.58359 [1.33927, 0.49802]
f_29: 8151.58335 [1.33972, 0.496955]
f_30: 8151.58351 [1.34041, 0.497255]
f_31: 8151.5834 [1.33957, 0.49622]
f_32: 8151.58348 [1.34031, 0.496495]
f_33: 8151.58334 [1.33956, 0.496833]
f_34: 8151.58334 [1.33953, 0.496902]
f_35: 8151.58334 [1.33963, 0.496867]
f_36: 8151.58334 [1.33963, 0.496803]
f_37: 8151.58334 [1.33956, 0.496833]
f_1: 8151.58334 [0.208273, 0.0543791, 0.304089, -1.0165, -2.0218, -1.01344, 1.33956, 0.496833]
f_2: 8152.77924 [0.343415, 0.0543791, 0.304089, -1.0165, -2.0218, -1.01344, 1.33956, 0.496833]
f_3: 8152.40989 [0.208273, 0.0599636, 0.304089, -1.0165, -2.0218, -1.01344, 1.33956, 0.496833]
f_4: 8151.66517 [0.208273, 0.0543791, 0.36783, -1.0165, -2.0218, -1.01344, 1.33956, 0.496833]
f_5: 8151.80713 [0.208273, 0.0543791, 0.304089, -0.930653, -2.0218, -1.01344, 1.33956, 0.496833]
f_6: 8152.01341 [0.208273, 0.0543791, 0.304089, -1.0165, -1.93539, -1.01344, 1.33956, 0.496833]
f_7: 8151.92522 [0.208273, 0.0543791, 0.304089, -1.0165, -2.0218, -0.943146, 1.33956, 0.496833]
f_8: 8152.13673 [0.208273, 0.0543791, 0.304089, -1.0165, -2.0218, -1.01344, 1.38956, 0.496833]
f_9: 8151.94237 [0.208273, 0.0543791, 0.304089, -1.0165, -2.0218, -1.01344, 1.33956, 0.546833]
f_10: 8152.55362 [0.0731315, 0.0543791, 0.304089, -1.0165, -2.0218, -1.01344, 1.33956, 0.496833]
f_11: 8152.45791 [0.208273, 0.0487947, 0.304089, -1.0165, -2.0218, -1.01344, 1.33956, 0.496833]
f_12: 8151.74834 [0.208273, 0.0543791, 0.240348, -1.0165, -2.0218, -1.01344, 1.33956, 0.496833]
f_13: 8151.75339 [0.208273, 0.0543791, 0.304089, -1.10234, -2.0218, -1.01344, 1.33956, 0.496833]
f_14: 8151.54498 [0.208273, 0.0543791, 0.304089, -1.0165, -2.10821, -1.01344, 1.33956, 0.496833]
f_15: 8151.60826 [0.208273, 0.0543791, 0.304089, -1.0165, -2.0218, -1.08374, 1.33956, 0.496833]
f_16: 8152.10285 [0.208273, 0.0543791, 0.304089, -1.0165, -2.0218, -1.01344, 1.28956, 0.496833]
f_17: 8152.03137 [0.208273, 0.0543791, 0.304089, -1.0165, -2.0218, -1.01344, 1.33956, 0.446833]
f_18: 8151.51379 [0.201253, 0.0544707, 0.313207, -1.02246, -2.0729, -1.04381, 1.33892, 0.499364]
f_19: 8151.50849 [0.201834, 0.0544477, 0.307696, -1.01938, -2.0686, -1.03218, 1.33889, 0.499004]
f_20: 8151.50725 [0.202301, 0.0544428, 0.307435, -1.01918, -2.06521, -1.02573, 1.33894, 0.498847]
f_21: 8151.50162 [0.202016, 0.0544549, 0.313046, -1.02218, -2.06781, -1.02509, 1.33895, 0.499106]
f_22: 8151.49719 [0.201869, 0.0544511, 0.323801, -1.02524, -2.07338, -1.02274, 1.33896, 0.499171]
f_23: 8151.49776 [0.201428, 0.054456, 0.329556, -1.02585, -2.07693, -1.02338, 1.33892, 0.499332]
f_24: 8151.49833 [0.202362, 0.0544455, 0.322284, -1.01796, -2.06941, -1.02202, 1.33901, 0.498991]
f_25: 8151.47933 [0.21279, 0.0544572, 0.325473, -1.02599, -2.07775, -1.02353, 1.33891, 0.499369]
f_26: 8151.4726 [0.222467, 0.0544635, 0.329038, -1.02663, -2.07984, -1.02578, 1.33882, 0.49981]
f_27: 8151.47283 [0.225105, 0.0544704, 0.336044, -1.0282, -2.0841, -1.02757, 1.33882, 0.499864]
f_28: 8151.47847 [0.228848, 0.0544831, 0.327657, -1.02586, -2.0846, -1.02593, 1.33791, 0.502905]
f_29: 8151.46851 [0.224444, 0.0544557, 0.33183, -1.02696, -2.08144, -1.02799, 1.33933, 0.497571]
f_30: 8151.47013 [0.225486, 0.0544539, 0.332178, -1.02102, -2.08508, -1.03057, 1.33908, 0.495344]
f_31: 8151.47069 [0.223228, 0.0544499, 0.329744, -1.02617, -2.07695, -1.0269, 1.33935, 0.493755]
f_32: 8151.45968 [0.225148, 0.054447, 0.330422, -1.0322, -2.0856, -1.03097, 1.34008, 0.495691]
f_33: 8151.45447 [0.227992, 0.0544292, 0.328456, -1.04239, -2.09654, -1.03672, 1.34127, 0.493969]
f_34: 8151.45918 [0.229311, 0.0544325, 0.330086, -1.04413, -2.10154, -1.03828, 1.34138, 0.490434]
f_35: 8151.45638 [0.2284, 0.0544054, 0.32812, -1.04043, -2.09418, -1.03818, 1.34328, 0.495434]
f_36: 8151.47958 [0.229351, 0.0540475, 0.330137, -1.04418, -2.1017, -1.03833, 1.34139, 0.493772]
f_37: 8151.43503 [0.228357, 0.0549791, 0.327915, -1.04312, -2.09728, -1.03693, 1.34131, 0.494389]
f_38: 8151.42873 [0.231501, 0.0554914, 0.325634, -1.04948, -2.11058, -1.03573, 1.34117, 0.49519]
f_39: 8151.4412 [0.234693, 0.0558142, 0.328494, -1.0496, -2.11166, -1.03685, 1.3439, 0.493868]
f_40: 8151.42228 [0.229747, 0.0554225, 0.322082, -1.05074, -2.10709, -1.03865, 1.33851, 0.495817]
f_41: 8151.41519 [0.230876, 0.0554716, 0.320353, -1.05515, -2.09995, -1.05066, 1.33745, 0.495698]
f_42: 8151.41837 [0.231917, 0.0554831, 0.322185, -1.05672, -2.09334, -1.0619, 1.33756, 0.495119]
f_43: 8151.42394 [0.233136, 0.0555808, 0.32198, -1.05902, -2.10776, -1.05438, 1.33224, 0.495585]
f_44: 8151.41764 [0.236975, 0.0554898, 0.312582, -1.06553, -2.10014, -1.05299, 1.33766, 0.495093]
f_45: 8151.41525 [0.229295, 0.0557859, 0.319215, -1.05245, -2.09448, -1.04805, 1.3376, 0.495778]
f_46: 8151.41722 [0.231816, 0.055567, 0.318097, -1.05656, -2.09244, -1.051, 1.33635, 0.495196]
f_47: 8151.41829 [0.229246, 0.0553928, 0.323776, -1.05237, -2.09431, -1.04797, 1.3376, 0.49578]
f_48: 8151.41089 [0.23074, 0.0557502, 0.316397, -1.05472, -2.10042, -1.05426, 1.33876, 0.49653]
f_49: 8151.41198 [0.232381, 0.0558152, 0.311411, -1.05926, -2.10384, -1.05568, 1.33882, 0.497362]
f_50: 8151.41477 [0.228963, 0.0556417, 0.315424, -1.05848, -2.09421, -1.05104, 1.33883, 0.496554]
f_51: 8151.41292 [0.232189, 0.0557031, 0.317655, -1.04737, -2.10054, -1.05632, 1.33933, 0.494855]
f_52: 8151.41441 [0.22879, 0.0556312, 0.315329, -1.0514, -2.0936, -1.05682, 1.33883, 0.496556]
f_53: 8151.40914 [0.234485, 0.0559177, 0.316945, -1.05997, -2.10519, -1.05689, 1.33921, 0.496603]
f_54: 8151.40927 [0.235094, 0.0559259, 0.316847, -1.06104, -2.10611, -1.05686, 1.33936, 0.496869]
f_55: 8151.40911 [0.235022, 0.0559492, 0.317208, -1.06086, -2.10513, -1.05778, 1.33921, 0.496598]
f_56: 8151.40846 [0.233993, 0.0559595, 0.31682, -1.06114, -2.10514, -1.05791, 1.33928, 0.495859]
f_57: 8151.40805 [0.23308, 0.0559846, 0.316397, -1.06299, -2.10672, -1.05809, 1.33938, 0.494842]
f_58: 8151.40826 [0.233712, 0.0560214, 0.315647, -1.06535, -2.10885, -1.05843, 1.33934, 0.494775]
f_59: 8151.40819 [0.233483, 0.0561039, 0.316287, -1.06309, -2.10633, -1.05641, 1.33857, 0.495043]
f_60: 8151.40797 [0.232771, 0.0560063, 0.31632, -1.0629, -2.10645, -1.05641, 1.3392, 0.494474]
f_61: 8151.40763 [0.232275, 0.0560265, 0.316495, -1.06273, -2.1066, -1.05709, 1.33901, 0.494798]
f_62: 8151.40754 [0.229639, 0.0560254, 0.317213, -1.06343, -2.10694, -1.05686, 1.33875, 0.495405]
f_63: 8151.40735 [0.230323, 0.0560417, 0.316948, -1.06182, -2.10841, -1.05698, 1.33846, 0.495079]
f_64: 8151.40732 [0.23012, 0.0560411, 0.316791, -1.06115, -2.10633, -1.05758, 1.33844, 0.495028]
f_65: 8151.40726 [0.230333, 0.0560435, 0.316886, -1.06161, -2.10713, -1.05735, 1.33854, 0.494993]
f_66: 8151.40654 [0.229963, 0.0561, 0.317912, -1.06128, -2.10728, -1.05793, 1.33895, 0.494786]
f_67: 8151.4058 [0.229034, 0.0562062, 0.320031, -1.06027, -2.10748, -1.05919, 1.33958, 0.494575]
f_68: 8151.40722 [0.228071, 0.0562951, 0.324567, -1.0565, -2.10567, -1.06123, 1.33996, 0.49462]
f_69: 8151.40547 [0.229008, 0.0563671, 0.319119, -1.06173, -2.10802, -1.06108, 1.33942, 0.49495]
f_70: 8151.40546 [0.228315, 0.0564303, 0.319475, -1.06332, -2.1105, -1.06202, 1.34033, 0.494093]
f_71: 8151.40553 [0.227978, 0.0564628, 0.318999, -1.06404, -2.11044, -1.06335, 1.34014, 0.494105]
f_72: 8151.40555 [0.228982, 0.0564232, 0.318806, -1.06208, -2.10996, -1.06219, 1.34019, 0.494691]
f_73: 8151.40568 [0.229176, 0.0564338, 0.319563, -1.06334, -2.111, -1.06204, 1.3401, 0.494158]
f_74: 8151.4053 [0.227716, 0.0564417, 0.319327, -1.06335, -2.10982, -1.06192, 1.34031, 0.493973]
f_75: 8151.40528 [0.2272, 0.05643, 0.319197, -1.06317, -2.10818, -1.06203, 1.34021, 0.493859]
f_76: 8151.40483 [0.226687, 0.056436, 0.318983, -1.06317, -2.10933, -1.06129, 1.34005, 0.494141]
f_77: 8151.40463 [0.22538, 0.0565257, 0.318266, -1.0638, -2.11162, -1.0608, 1.33976, 0.493626]
f_78: 8151.40478 [0.225658, 0.0565514, 0.317663, -1.06627, -2.11296, -1.06078, 1.3403, 0.494195]
f_79: 8151.40472 [0.226072, 0.0566061, 0.318564, -1.06404, -2.11085, -1.06072, 1.33963, 0.493761]
f_80: 8151.40422 [0.225057, 0.0565605, 0.319044, -1.06377, -2.11106, -1.06114, 1.33987, 0.49411]
f_81: 8151.40362 [0.222827, 0.0566586, 0.320692, -1.06331, -2.11136, -1.06179, 1.33985, 0.494459]
f_82: 8151.40403 [0.219734, 0.0569046, 0.32365, -1.06214, -2.11065, -1.06379, 1.34005, 0.494269]
f_83: 8151.40448 [0.224243, 0.0567073, 0.320273, -1.06211, -2.11313, -1.06252, 1.34036, 0.494582]
f_84: 8151.40332 [0.221408, 0.0566446, 0.322561, -1.0625, -2.11134, -1.0605, 1.3397, 0.494653]
f_85: 8151.40353 [0.221186, 0.0565914, 0.323377, -1.06032, -2.11111, -1.05956, 1.33981, 0.494786]
f_86: 8151.40322 [0.22033, 0.0567388, 0.323052, -1.06402, -2.11242, -1.06092, 1.33974, 0.494633]
f_87: 8151.40357 [0.220632, 0.0567788, 0.322528, -1.06629, -2.11265, -1.06176, 1.33921, 0.494429]
f_88: 8151.40378 [0.221899, 0.0567197, 0.323678, -1.06413, -2.11268, -1.06124, 1.33976, 0.494588]
f_89: 8151.40293 [0.219314, 0.0567806, 0.32276, -1.06408, -2.11208, -1.06046, 1.34012, 0.494394]
f_90: 8151.40262 [0.21678, 0.0569205, 0.323197, -1.06451, -2.11257, -1.06052, 1.34026, 0.494228]
f_91: 8151.40277 [0.212945, 0.0571722, 0.3245, -1.06387, -2.11531, -1.06005, 1.33995, 0.494403]
f_92: 8151.40291 [0.21517, 0.0570524, 0.323317, -1.06505, -2.11304, -1.06112, 1.34105, 0.494095]
f_93: 8151.40286 [0.214719, 0.0570045, 0.32411, -1.06452, -2.11304, -1.06009, 1.33983, 0.493364]
f_94: 8151.40256 [0.216858, 0.0569259, 0.323662, -1.06533, -2.11224, -1.06057, 1.34013, 0.49475]
f_95: 8151.40233 [0.21581, 0.0569865, 0.32359, -1.06507, -2.11286, -1.06032, 1.34006, 0.49498]
f_96: 8151.40207 [0.213479, 0.0571208, 0.323625, -1.06567, -2.11342, -1.06014, 1.3399, 0.495524]
f_97: 8151.40239 [0.209969, 0.0573482, 0.324326, -1.06736, -2.11418, -1.06054, 1.33973, 0.495827]
f_98: 8151.40231 [0.214165, 0.0571102, 0.323429, -1.06678, -2.11459, -1.0594, 1.34014, 0.495325]
f_99: 8151.40175 [0.211207, 0.0571651, 0.324385, -1.06406, -2.11271, -1.05943, 1.34026, 0.495837]
f_100: 8151.40196 [0.2081, 0.0572868, 0.32591, -1.06348, -2.11262, -1.05892, 1.34064, 0.496062]
f_101: 8151.40194 [0.209078, 0.0571148, 0.324028, -1.06445, -2.11324, -1.05922, 1.34035, 0.495707]
f_102: 8151.40146 [0.210362, 0.0571892, 0.323546, -1.06424, -2.11263, -1.05941, 1.3402, 0.495854]
f_103: 8151.40111 [0.208328, 0.0572922, 0.323131, -1.06442, -2.11223, -1.05866, 1.34022, 0.495143]
f_104: 8151.40115 [0.205928, 0.0574251, 0.323074, -1.06446, -2.11282, -1.0585, 1.34037, 0.495289]
f_105: 8151.4011 [0.207997, 0.0572982, 0.323115, -1.06425, -2.11215, -1.0591, 1.33957, 0.494936]
f_106: 8151.40097 [0.207305, 0.0573195, 0.323289, -1.06386, -2.11121, -1.05915, 1.33991, 0.494862]
f_107: 8151.40095 [0.205623, 0.0573649, 0.324192, -1.06242, -2.10997, -1.05907, 1.34018, 0.494683]
f_108: 8151.40104 [0.204215, 0.0574516, 0.323811, -1.0644, -2.11026, -1.05933, 1.34012, 0.494539]
f_109: 8151.40093 [0.204211, 0.0573531, 0.323977, -1.06234, -2.11022, -1.0595, 1.34055, 0.494869]
f_110: 8151.40078 [0.205106, 0.0573623, 0.323485, -1.06188, -2.10952, -1.05984, 1.34042, 0.495007]
f_111: 8151.40075 [0.206356, 0.0572969, 0.322777, -1.06088, -2.10797, -1.06065, 1.34039, 0.495212]
f_112: 8151.40076 [0.206198, 0.0573224, 0.322862, -1.06125, -2.10886, -1.06045, 1.34032, 0.495185]
f_113: 8151.40061 [0.206117, 0.057282, 0.322701, -1.0603, -2.10811, -1.06029, 1.34022, 0.495247]
f_114: 8151.4005 [0.205622, 0.057283, 0.322747, -1.05985, -2.10794, -1.05988, 1.34012, 0.495453]
f_115: 8151.40048 [0.204723, 0.0572887, 0.32312, -1.05855, -2.10751, -1.0594, 1.34014, 0.495675]
f_116: 8151.40044 [0.204582, 0.0572965, 0.322707, -1.05847, -2.10735, -1.05919, 1.34043, 0.495517]
f_117: 8151.40025 [0.204174, 0.0572871, 0.322429, -1.05883, -2.107, -1.05894, 1.3402, 0.495585]
f_118: 8151.40008 [0.203058, 0.0572919, 0.321749, -1.05917, -2.10608, -1.05871, 1.33982, 0.495362]
f_119: 8151.40035 [0.204673, 0.0571722, 0.320158, -1.05925, -2.10445, -1.05889, 1.33967, 0.495413]
f_120: 8151.40031 [0.202852, 0.0572944, 0.321712, -1.05957, -2.10573, -1.05952, 1.34022, 0.495456]
f_121: 8151.39992 [0.202005, 0.0573629, 0.321335, -1.0592, -2.10641, -1.05804, 1.33961, 0.495539]
f_122: 8151.39986 [0.198106, 0.0574857, 0.321766, -1.05857, -2.10607, -1.05713, 1.33964, 0.495552]
f_123: 8151.39994 [0.198128, 0.0574939, 0.321626, -1.05825, -2.10633, -1.05726, 1.33971, 0.495953]
f_124: 8151.39983 [0.198228, 0.0574768, 0.321859, -1.0585, -2.10559, -1.05699, 1.33965, 0.495439]
f_125: 8151.39983 [0.198263, 0.0574678, 0.322212, -1.05826, -2.10527, -1.05661, 1.33973, 0.49562]
f_126: 8151.40008 [0.197382, 0.0574358, 0.321767, -1.05846, -2.10558, -1.05712, 1.33964, 0.495492]
f_127: 8151.39992 [0.198444, 0.0575262, 0.322018, -1.05837, -2.10539, -1.05713, 1.3397, 0.495415]
f_128: 8151.39985 [0.198245, 0.0574751, 0.321802, -1.05777, -2.10585, -1.05694, 1.33968, 0.495231]
f_129: 8151.39978 [0.198581, 0.0574822, 0.321491, -1.05859, -2.10563, -1.05646, 1.33969, 0.495424]
f_130: 8151.39985 [0.198709, 0.0574832, 0.321522, -1.05842, -2.10561, -1.05631, 1.33921, 0.495452]
f_131: 8151.39978 [0.197931, 0.0575049, 0.321334, -1.05822, -2.10516, -1.05629, 1.33965, 0.495407]
f_132: 8151.39978 [0.199085, 0.0574494, 0.321678, -1.05841, -2.10521, -1.05649, 1.33969, 0.495229]
f_133: 8151.3998 [0.199215, 0.0574692, 0.321785, -1.05891, -2.10609, -1.05663, 1.33975, 0.495467]
f_134: 8151.39979 [0.19902, 0.0574584, 0.321444, -1.05807, -2.10516, -1.05656, 1.33969, 0.495509]
f_135: 8151.3998 [0.199083, 0.0574795, 0.321698, -1.0591, -2.10604, -1.05672, 1.33973, 0.495346]
f_136: 8151.39977 [0.197648, 0.0575089, 0.321553, -1.05856, -2.1056, -1.05612, 1.33972, 0.495319]
f_137: 8151.39979 [0.196938, 0.057546, 0.321322, -1.0585, -2.10561, -1.05598, 1.33975, 0.495479]
f_138: 8151.39977 [0.197933, 0.0574944, 0.321558, -1.05854, -2.10535, -1.0562, 1.33968, 0.49523]
f_139: 8151.39977 [0.198156, 0.0574865, 0.321526, -1.05867, -2.10536, -1.05629, 1.33967, 0.495211]
f_140: 8151.39977 [0.198183, 0.0574865, 0.321336, -1.05906, -2.10525, -1.05637, 1.33965, 0.495123]
f_141: 8151.39976 [0.198381, 0.0574756, 0.321478, -1.05869, -2.10533, -1.05638, 1.33967, 0.495218]
f_142: 8151.39976 [0.198521, 0.0574666, 0.321407, -1.0588, -2.10535, -1.05642, 1.33964, 0.495158]
f_143: 8151.39976 [0.198441, 0.0574689, 0.32139, -1.05881, -2.10532, -1.05639, 1.33963, 0.495153]
f_144: 8151.39976 [0.198421, 0.0574676, 0.321384, -1.05882, -2.10526, -1.05637, 1.33964, 0.495186]
f_145: 8151.39976 [0.198387, 0.057467, 0.321343, -1.05881, -2.10513, -1.05632, 1.33964, 0.49523]
f_146: 8151.39975 [0.198313, 0.0574654, 0.321223, -1.05882, -2.10487, -1.05621, 1.33964, 0.495268]
f_147: 8151.39975 [0.197982, 0.0574693, 0.320987, -1.0588, -2.10446, -1.05598, 1.33962, 0.495262]
f_148: 8151.39976 [0.198343, 0.0574653, 0.321326, -1.0588, -2.10475, -1.05621, 1.33964, 0.495174]
f_149: 8151.39975 [0.198315, 0.0574626, 0.321082, -1.05895, -2.10476, -1.05615, 1.33963, 0.495341]
f_150: 8151.39976 [0.198324, 0.0574708, 0.321293, -1.05878, -2.10492, -1.05629, 1.33964, 0.495281]
f_151: 8151.39975 [0.198201, 0.0574646, 0.321207, -1.05886, -2.10492, -1.0561, 1.33964, 0.495287]
f_152: 8151.39975 [0.197853, 0.0574753, 0.321153, -1.0589, -2.10493, -1.05595, 1.33965, 0.495302]
f_153: 8151.39975 [0.198197, 0.0574636, 0.321154, -1.05896, -2.10484, -1.05603, 1.33964, 0.495292]
f_154: 8151.39975 [0.198159, 0.0574628, 0.321144, -1.05881, -2.10483, -1.05599, 1.33962, 0.495286]
f_155: 8151.39974 [0.198195, 0.0574584, 0.321067, -1.05863, -2.10474, -1.05585, 1.33957, 0.495297]
f_156: 8151.39974 [0.198118, 0.0574497, 0.320907, -1.05831, -2.1044, -1.0556, 1.33952, 0.495297]
f_157: 8151.39976 [0.198001, 0.0574524, 0.321048, -1.05867, -2.10476, -1.05589, 1.3395, 0.495274]
f_158: 8151.39974 [0.198213, 0.0574637, 0.320918, -1.05868, -2.10472, -1.05569, 1.33961, 0.49529]
f_159: 8151.39974 [0.198093, 0.057466, 0.320824, -1.05867, -2.10464, -1.05557, 1.3396, 0.495287]
f_160: 8151.39974 [0.198367, 0.0574632, 0.32094, -1.05882, -2.10466, -1.05564, 1.33954, 0.495286]
f_161: 8151.39973 [0.198205, 0.0574647, 0.320849, -1.0586, -2.1048, -1.05573, 1.33965, 0.495278]
f_162: 8151.39973 [0.198311, 0.057461, 0.320707, -1.05857, -2.10474, -1.05575, 1.33962, 0.495232]
f_163: 8151.39973 [0.198154, 0.0574629, 0.32078, -1.05862, -2.10471, -1.05567, 1.33969, 0.49526]
f_164: 8151.39973 [0.198104, 0.0574693, 0.320809, -1.0586, -2.10489, -1.05571, 1.3397, 0.495268]
f_165: 8151.39973 [0.19785, 0.0574833, 0.320755, -1.05863, -2.10506, -1.05565, 1.33973, 0.495269]
f_166: 8151.39973 [0.198094, 0.057467, 0.320738, -1.05854, -2.10484, -1.05562, 1.33971, 0.495287]
f_167: 8151.39973 [0.198058, 0.0574681, 0.320705, -1.05841, -2.10492, -1.05561, 1.33973, 0.495313]
f_168: 8151.39973 [0.198125, 0.0574682, 0.32064, -1.05854, -2.10496, -1.05558, 1.33974, 0.495319]
f_169: 8151.39973 [0.198179, 0.05747, 0.320604, -1.0587, -2.10509, -1.0556, 1.33975, 0.495333]
f_170: 8151.39973 [0.198159, 0.0574694, 0.320603, -1.05865, -2.10505, -1.05558, 1.33975, 0.495325]
f_171: 8151.39973 [0.198156, 0.057469, 0.320576, -1.05866, -2.10506, -1.05556, 1.33975, 0.495312]
f_172: 8151.39973 [0.198117, 0.0574695, 0.320537, -1.05868, -2.10505, -1.05551, 1.33975, 0.495303]
f_173: 8151.39973 [0.198213, 0.0574664, 0.320592, -1.05863, -2.10511, -1.05556, 1.33975, 0.495298]
f_174: 8151.39973 [0.19829, 0.0574628, 0.320652, -1.05862, -2.10524, -1.05557, 1.33975, 0.495303]
f_175: 8151.39973 [0.198369, 0.0574648, 0.320753, -1.0587, -2.10552, -1.05562, 1.33977, 0.495347]
f_176: 8151.39972 [0.198326, 0.0574586, 0.320584, -1.05866, -2.10525, -1.05549, 1.33976, 0.495292]
f_177: 8151.39972 [0.198505, 0.0574534, 0.320608, -1.05873, -2.10526, -1.05553, 1.33974, 0.4953]
f_178: 8151.39972 [0.198869, 0.0574432, 0.320569, -1.05887, -2.10535, -1.05559, 1.33971, 0.495303]
f_179: 8151.39972 [0.198992, 0.0574377, 0.320544, -1.05894, -2.10537, -1.0555, 1.33971, 0.495318]
f_180: 8151.39972 [0.198946, 0.0574396, 0.320566, -1.0589, -2.10537, -1.05553, 1.33971, 0.495315]
f_181: 8151.39972 [0.199023, 0.057436, 0.320593, -1.05877, -2.10536, -1.05553, 1.33971, 0.495335]
f_182: 8151.39972 [0.198997, 0.0574374, 0.320616, -1.05879, -2.10538, -1.05553, 1.33971, 0.495325]
f_183: 8151.39972 [0.199028, 0.057435, 0.320662, -1.05878, -2.10536, -1.05555, 1.3397, 0.495311]
f_184: 8151.39972 [0.199062, 0.0574328, 0.320669, -1.05878, -2.10534, -1.05555, 1.3397, 0.495307]
f_185: 8151.39972 [0.19898, 0.0574293, 0.320661, -1.05879, -2.10536, -1.05557, 1.33969, 0.495321]
f_186: 8151.39972 [0.199112, 0.0574294, 0.320654, -1.05881, -2.10536, -1.05554, 1.33969, 0.495308]
f_187: 8151.39972 [0.199112, 0.0574284, 0.320652, -1.05881, -2.1054, -1.05555, 1.33974, 0.495299]
f_188: 8151.39972 [0.199136, 0.0574295, 0.320653, -1.05885, -2.10541, -1.05555, 1.33971, 0.495312]
f_189: 8151.39972 [0.199101, 0.0574291, 0.320643, -1.05879, -2.10538, -1.05552, 1.33971, 0.495303]
f_190: 8151.39972 [0.199083, 0.0574289, 0.320653, -1.05883, -2.10536, -1.05549, 1.33973, 0.495336]
f_191: 8151.39972 [0.199078, 0.0574291, 0.320646, -1.05878, -2.10537, -1.05549, 1.33971, 0.49531]
f_192: 8151.39972 [0.19915, 0.0574275, 0.320619, -1.05878, -2.10541, -1.05547, 1.33973, 0.495313]
f_193: 8151.39972 [0.199023, 0.0574317, 0.320669, -1.05878, -2.10531, -1.0555, 1.3397, 0.495308]
f_194: 8151.39972 [0.199142, 0.0574264, 0.320641, -1.05879, -2.10542, -1.05547, 1.33972, 0.495317]
f_195: 8151.39972 [0.19909, 0.0574288, 0.32066, -1.05879, -2.10538, -1.0555, 1.33971, 0.49531]
f_196: 8151.39972 [0.19909, 0.0574292, 0.320675, -1.05879, -2.1054, -1.0555, 1.33971, 0.495311]
f_197: 8151.39972 [0.199089, 0.0574294, 0.320678, -1.05879, -2.1054, -1.0555, 1.33971, 0.495311]
  1.876459 seconds (428.06 k allocations: 44.514 MiB, 9.01% gc time)
Generalized Linear Mixed Model fit by minimizing the Laplace approximation to the deviance
  Formula: r2 ~ 1 + a + g + b + s + (1 | id) + (1 | item)
  Distribution: Distributions.Bernoulli{Float64}
  Link: GLM.LogitLink()

  Deviance (Laplace approximation): 8151.3997

Variance components:
          Column    Variance   Std.Dev.  
 id   (Intercept)  1.79482666 1.33971141
 item (Intercept)  0.24533278 0.49531079

 Number of obs: 7584; levels of grouping factors: 316, 24

Fixed-effects parameters:
              Estimate Std.Error  z value P(&gt;|z|)
(Intercept)   0.199089   0.40518 0.491359  0.6232
a            0.0574294 0.0167573  3.42712  0.0006
g: M          0.320678  0.191259  1.67667  0.0936
b: scold      -1.05879  0.256808 -4.12288   &lt;1e-4
b: shout       -2.1054  0.258532 -8.14369  &lt;1e-15
s: self        -1.0555  0.210305 -5.01891   &lt;1e-6

</code></pre><p>This fit provided slightly better results (Laplace approximation to the deviance of 8151.400 versus 8151.583) but took 6 times as long. That is not terribly important when the times involved are a few seconds but can be important when the fit requires many hours or days of computing time.</p><p>The comparison of the slow and fast fit is available in the optimization summary after the slow fit.</p><pre><code class="language-julia">julia&gt; mdl1.LMM.optsum
Initial parameter vector: [0.208273, 0.0543791, 0.304089, -1.0165, -2.0218, -1.01344, 1.33956, 0.496833]
Initial objective value:  8151.583340132033

Optimizer (from NLopt):   LN_BOBYQA
Lower bounds:             [-Inf, -Inf, -Inf, -Inf, -Inf, -Inf, 0.0, 0.0]
ftol_rel:                 1.0e-12
ftol_abs:                 1.0e-8
xtol_rel:                 0.0
xtol_abs:                 [1.0e-10, 1.0e-10]
initial_step:             [0.135142, 0.00558444, 0.0637411, 0.0858438, 0.0864116, 0.0702961, 0.05, 0.05]
maxfeval:                 -1

Function evaluations:     197
Final parameter vector:   [0.199089, 0.0574294, 0.320678, -1.05879, -2.1054, -1.0555, 1.33971, 0.495311]
Final objective value:    8151.399719814065
Return code:              FTOL_REACHED

</code></pre><footer><hr/><a class="previous" href="MultipleTerms.html"><span class="direction">Previous</span><span class="title">Models With Multiple Random-effects Terms</span></a><a class="next" href="SingularCovariance.html"><span class="direction">Next</span><span class="title">Singular covariance estimates in random regression models</span></a></footer></article></body></html>
